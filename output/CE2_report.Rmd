---
title: "CE2_Martinoli_Travaglini"
author: "Francesca Martinoli, Simone Travaglini"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 1: inspect the *robots.txt*

Inspecting the **robots.txt** page for the blog [beppegrillo.it](https://beppegrillo.it/), we can see that it returns a 404 error (which means that the page it's "not found"), because **beppegrillo.it** does not have a robots.txt page. In such situation we can proceed with the scraping using a politely method, which includes:

-   Be identifiable with ***User-Agent*** and ***From*** header fields

-   Scrape as few as possible

-   Do not bombard the server

## Task 2: download the page in a politely way

```{r, eval = F}
library(RCurl)
url <- "https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/"
resp <- RCurl::getURL(url = url, httpheader = c(from = "my@email.com", 'User-Agent' = R.Version()$version.string))

```

There are many ways to get and download pages from the web using R. We used the function [*getURL*]{.underline} from the package [**RCurl**]{.underline}. First of all, we create an object called ***url*** which contains the link to the page ["un mare di plastica ci sommergerÃ "](https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/) from **beppegrillo.it**. Then we used this object into the function ***getURL***, in which we added an header which contains our email and our 'User-Agent' in order to making a politely request. We stored the response at this request into an object called ***resp***.

## Task 3: get all the HTML links and keep only those which re-direct to other post of blog

First, we got all the links from the page (stored into ***resp***) using the function [*getHTMLLinks*]{.underline} from the package [**XML**]{.underline}. 

```{r, eval = F}
library(XML)
links <- XML::getHTMLLinks(resp, externalOnly = T)
```
We stored the links into an object called ***links***, then we transformed it into a data frame (called ***data***), using the function [**tibble**]{.underline}. 

```{r, eval = F}
data <- tibble(links)
```
Then, we need to keep only those links that re-direct to other posts of the **beppegrillo.it** blog. In order to do this, we used a regular expression into the function **stringr::str_extract_all** that match only the links we need: this regex allows us to match only the links of other post and not e.g. the links for the sections of the blog. 

```{r, eval = F}
library(stringr)
link_grillo <- str_extract_all(
  links_unl, pattern = "^https://beppegrillo\\.it/[^category].+")

link_grillo <- unlist(link_grillo)
link_grillo2 <- as.data.frame(link_grillo)

```
The regular expression used for extract the links is **^https://beppegrillo\\.it/[^category].+**.

We stored the result into an object called ***link_grillo*** and then we transofmed it into a data frame using the function [*as.data.frame*]{.underline}.


## Task 4: scrape all the posts for 2016

First of all we create a new function [*download_politely*]{.underline} that allows us to get and download pages in a politely way.

```{r, eval = F}
download_politely <- function(from_url, to_html, my_email, my_agent = R.Version()$version.string) {
  
  require(httr)
  
  # Check that arguments are inputted as expected:
  stopifnot(is.character(from_url))
  stopifnot(is.character(to_html))
  stopifnot(is.character(my_email))
  
  # GET politely
  grillo_16 <- httr::GET(url = from_url, 
                         add_headers(
                           From = my_email, 
                           `User-Agent` = R.Version()$version.string
                         )
  )
  # If status == 200, extract content and save to a file:
  if (httr::http_status(grillo_16)$message == "Success: (200) OK") {
    bin <- content(grillo_16, as = "raw")
    writeBin(object = bin, con = to_html)
  } else {
    cat("Houston, we have a problem!")
  }
}
```
Then we crated a vector with the links of the 47 pages of the archive for 2016, and we used it into a for loop to download all the links into a folder called ***grillo_linkpages***, using the function [*download_politely]{.underline}.Within link we set the url "https://beppegrillo.it/category/archivio/2016/page/" and set 1:47, which is the total number of pages in the 2016 archive.

```{r, eval = F}
link <- str_c("https://beppegrillo.it/category/archivio/2016/page/", 1:47)

dir.create("grillo_linkpages")

for (i in seq_along(link)) {
  cat(i, " ")
  
  download_politely(from_url = link[i], 
                    to_html = here::here("grillo_linkpages", str_c("page_",i,".html")), 
                    my_email = "francescamartinoli@yahoo.com")
  
  Sys.sleep(0.5)
}

```
In order to get and download the 47 pages we set **Sys.sleep(0.5)**. 

The next step require us to select the main text from every page. 
To do this, we had to create a list with all the links to scrape, which we called to_scrape, and an empty vector that works as a container for all article links, called all_title. 

```{r, eval = F}
to_scrape <- list.files(here::here("grillo_linkpages"), full.names = TRUE)    
all_title <- vector(mode = "list", length = length(to_scrape))
```

after that, we create a loop in order to select all the title from the all pages conteined in the list "to_scrape". we used the selector gadget, but we realised that this was not the right method. with the selector gadget we could in fact only select the article titles on each page, but not the title links. so we need to find another way to do it.
```{r, eval = F}
for (i in seq_along(all_title)){
  all_title[[i]] <- read_html(to_scrape[i]) %>% 
    html_elements(css = ".td_module_10 .td-module-title") %>% 
    html_text(trim = TRUE)
}

```
in order to get the links, we tryied to use the GetHTMLLinks function from XML. so again, we create an empty vector called all_link and set a loop to extract links from all the pages of the 2016 archive.
finally we get a list with all the links for every page of the archive and then we used the function unlist. 
```{r, eval = F}
all_link <- vector(mode = "list", length = length(to_scrape))    # empty container where to place the posts

for (i in seq_along(all_link)){
  all_link[[i]] <- XML::getHTMLLinks(to_scrape[i], externalOnly = T)
}
all_link <- unlist(all_link)

all_link
```
to match only the links we need, we set a regular expression into the function **stringr::str_extract_all**: the regular expression was ***"https://beppegrillo\\.it/[^category][^jpg].+"***
At last, we used the function unlist on **post2016** and create a list called post_2016_lst which contain the links of all the posts of 2016.
```{r, eval = F}
post2016 <- str_extract_all(all_link, pattern = "https://beppegrillo\\.it/[^category][^jpg].+")
post2016 <- unlist(post2016)
post2016 <- tibble(post2016)
post2016 <- distinct(post2016)
```
```{r}
post_2016_lst <- unlist(post2016)
dir.create("link_post2016")

for (i in seq_along(post_2016_lst)) {
  cat(i, " ")
  
  download_politely(from_url = post_2016_lst[i], 
                    to_html = here::here("link_post2016", str_c("post_",i,".html")), 
                    my_email = "francescamartinoli@yahoo.com")
  
  Sys.sleep(0.5)
}
```

the last part of the task 4 was select the main text from all the links. in order to do it, we use the the selector gadget. At the end, we used the function distinct to remove all the duplication.
```{r, eval=F}
library(rvest)
library(stringr)

text_scrape <- list.files(here::here("link_post2016"), full.names = TRUE)


text_post = lapply(text_scrape, function(x) {
  read = read_html(x) %>% 
    html_elements(css = "p") %>% 
    html_text(trim = TRUE)
})

print(text_post[[1]])

text_post_unl <- unlist(text_post)
text_post_t <- tibble(text_post_unl)
text_post_t <- distinct(text_post_t) #ripulito

```

## Task 5

